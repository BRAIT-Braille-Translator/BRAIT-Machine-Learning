{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffa7023e-49eb-4227-8da2-7efd1a4e93a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torch.onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b5bd7422-1bf2-4abb-b539-520c12c54d9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class BRAIT_CNN(nn.Module):\n",
    "  def __init__(self):\n",
    "    super(BRAIT_CNN, self).__init__()\n",
    "    self.brait1 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout(p=0.01))\n",
    "    self.brait2 = nn.Sequential(\n",
    "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n",
    "        nn.MaxPool2d(kernel_size=2),\n",
    "        nn.Dropout(p=0.01))\n",
    "    self.brait3 = nn.Sequential(\n",
    "        nn.Linear(32*7*7, 100),\n",
    "        nn.Linear(100, 26))\n",
    "\n",
    "  def forward(self, x):\n",
    "    y = F.relu(self.brait1(x))\n",
    "    y = F.relu(self.brait2(y))\n",
    "\n",
    "    #flatten\n",
    "    y = y.view(-1, 32*7*7)\n",
    "    y = F.relu(self.brait3(y))\n",
    "\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "530beab1-3b0e-40c0-ab85-ac4c4bb765b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_model():\n",
    "    model = CNN()\n",
    "    model.load_state_dict(torch.load(\"D:\\BRAIT-ML\\BRAIT-WEIGHT\\BRAIT_PYTORCH.pth\"))\n",
    "\n",
    "    # Input to the model\n",
    "    x = torch.randn(5, 3, 28, 28)\n",
    "\n",
    "    # Export the model\n",
    "    torch_out = torch.onnx._export(model,  # model being run\n",
    "                                   x,  # model input (or a tuple for multiple inputs)\n",
    "                                   \"model.onnx-2\",\n",
    "                                   # where to save the model (can be a file or file-like object)\n",
    "                                   export_params=True)  # store the trained parameter weights inside the model file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b12d4dae-07b7-4d76-af24-b57c0c1b998e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inspect_model():\n",
    "    # Input image into the ONNX model\n",
    "    onnx_model = onnx.load(\"D:\\BRAIT-ML\\BRAIT-WEIGHT\\BRAIT.onnx\")\n",
    "    model = onnx_caffe2.backend.prepare(onnx_model)\n",
    "\n",
    "    image = Image.open(\"z.jpg\")\n",
    "    # # image = image.convert('RGB')\n",
    "    image = np.array(image)\n",
    "    image = cv2.resize(image, (28, 28))\n",
    "    image = image.astype(np.float32) / 255.0\n",
    "    image = torch.from_numpy(image[None, :, :, :])\n",
    "    image = image.permute(0, 3, 1, 2)\n",
    "    W = {model.graph.input[0].name: image.data.numpy()}\n",
    "    model_out = model.run(W)[0]\n",
    "    print(model_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cb58dd0-767f-4dd5-92e5-a36e07660c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "31.0\n",
      "['f', 'a', 'm', 'i', 'l', 'y']\n",
      "4\n",
      "31.5\n",
      "['h', 'r', 'm', 'e']\n",
      "7\n",
      "76.28571428571429\n",
      "['p', 'r', 'a', 'i', 'r', 'i', 'e']\n",
      "14\n",
      "32.07142857142857\n",
      "['t', 'h', 'r', 'e', 'w', 's', 't', 'a', 'e', 'c', 'a', 'a', 'l', 'l']\n",
      "5\n",
      "30.8\n",
      "['w', 'o', 'u', 'l', 'd']\n",
      "15\n",
      "31.6\n",
      "['w', 'i', 't', 'h', 'k', 'h', 'i', 's', 'c', 'f', 'a', 'm', 'i', 'l', 'y']\n",
      "10\n",
      "31.2\n",
      "['t', 'h', 'e', 'a', 'l', 'i', 't', 't', 'l', 'd']\n",
      "11\n",
      "32.18181818181818\n",
      "['l', 'i', 't', 't', 'l', 'e', 'z', 'g', 'i', 'a', 'l']\n",
      "1\n",
      "386.0\n",
      "['p']\n"
     ]
    }
   ],
   "source": [
    "def BRAIT_PREDICTION(img_path):\n",
    "    #load model BRAIT_CNN\n",
    "    model = BRAIT_CNN()\n",
    "    model.load_state_dict(torch.load('D:\\\\BRAIT-ML\\\\BRAIT-WEIGHT\\\\BRAIT_PYTORCH.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "    #prepocess image inputan convert jadi RGB\n",
    "    image = Image.open(img_path)\n",
    "    image = image.convert('RGB')\n",
    "\n",
    "    #segmentasi image inputan\n",
    "    width, height = image.size #mengambil ukuran size image\n",
    "    jumlah_segment = round(width/height/0.78) #menentukan jumlah segment huruf braille\n",
    "    print(jumlah_segment)\n",
    "    segment = width/jumlah_segment\n",
    "    print(segment)\n",
    "    \n",
    "    tamp=[]\n",
    "    for i in range (0,jumlah_segment):\n",
    "        cropped = image.crop((i*segment,0,(i+1)*segment,height))\n",
    "        cropped = np.array(cropped)\n",
    "        cropped = cv2.resize(cropped, (28, 28))\n",
    "        cropped = cropped.astype(np.float32) / 255.0\n",
    "        cropped = torch.from_numpy(cropped[None, :, :, :])\n",
    "        cropped = cropped.permute(0, 3, 1, 2)\n",
    "        predicted_tensor = model(cropped)\n",
    "        _, predicted_letter = torch.max(predicted_tensor, 1)\n",
    "        if int(predicted_letter) == 26:\n",
    "            tamp.append(chr(32))\n",
    "        else:\n",
    "            tamp.append(chr(97 + predicted_letter))     \n",
    "        \n",
    "    return tamp\n",
    "\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\family.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\home.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\Prairie.jpg\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\threw_the_ball.png\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\would.png\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\with_his_family.png\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\the_little.png\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\BRAIT-SAMPLE\\little_girl.png\"))\n",
    "print(BRAIT_PREDICTION(r\"D:\\BRAIT-ML\\dataset-BRAIT\\dataset\\test\\i\\P_20180609_101612_2_2_1.jpg\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24c2060-95ad-462b-a2bd-2aefce41707a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
